{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76936723-4629-4307-84bc-c9cc8b417813",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951a01d0-9295-4616-a6f8-337724ce93ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "min max scaling also known as normalisation  it data preprocessing technique used to scale and transform \n",
    "feature in a dataset so that they have in a specific range.main goal of this scaling is bring all  feature \n",
    " within in certain range typically in between 0 and 1.\n",
    "    X(scaled)=[X(i)-X(min)]/[X(max)-X(min)]\n",
    " \n",
    "Example:\n",
    "Suppose we have a dataset of house prices with a feature \"Area\" representing the size of the houses and another feature \n",
    "\"Rooms\" representing the number of rooms in each house. We want to apply Min-Max scaling to both of these features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ce4c30-a52c-4241-9b52-d913c453d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd951e61-642f-4d64-968b-1cd735378d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "The \"Unit Vector\" technique in feature scaling, also known as \"Normalization\" or \"L2 normalization,\" is a method used to scale\n",
    "numerical features in a dataset to have a unit norm (length of 1). This technique involves dividing each feature by the Euclidean\n",
    "norm (L2 norm) of the feature vector. The resulting scaled feature vector will lie on the surface of a unit hypersphere.\n",
    "\n",
    "Mathematically, for a feature vector x, the normalized feature vector x_normalized is calculated as:\n",
    "\n",
    "x_normalized = x / ||x||\n",
    "\n",
    "Where ||x|| represents the Euclidean norm of the vector x, which is given by:\n",
    "\n",
    "||x|| = sqrt(x[1]^2 + x[2]^2 + ... + x[n]^2)\n",
    "\n",
    "Here, n is the number of features in the vector x.\n",
    "\n",
    "On the other hand, Min-Max scaling is a different technique that scales features to a specific range, typically between 0 and 1.\n",
    "It is done by subtracting the minimum value of the feature and then dividing by the range (difference between maximum and minimum values).\n",
    "\n",
    "Mathematically, for a feature vector x, the Min-Max scaled feature vector x_scaled is calculated as:\n",
    "\n",
    "x_scaled = (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "Difference:-\n",
    "The main difference between Unit Vector scaling and Min-Max scaling lies in their approach to normalization. Unit Vector scaling normalizes the \n",
    "entire feature vector, ensuring that each feature has unit norm, which is useful when the direction of the feature matters more than its magnitude. Min-Max scaling, on the other hand, scales each feature individually to a specific range, which can help when you want to bring all features to a common scale for modeling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a3f921-cea7-466f-9246-2b33a2dfa9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f3765-bff1-4ca9-b704-69c2bd62be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Principal Component Analysis (PCA) is a widely used technique in the field of dimensionality reduction and data compression. It aims to transform high-dimensional data into a new coordinate system while preserving the most important information or variability in the data. PCA achieves this by identifying the orthogonal axes (principal components) along which the data has the highest variance.\n",
    "Here's how PCA works and how it is used in dimensionality reduction:\n",
    "\n",
    "Covariance Matrix Calculation: Given a dataset with multiple features, PCA starts by calculating the covariance matrix of the features. The covariance matrix describes the relationships between different features and how they vary together.\n",
    "\n",
    "Eigenvector and Eigenvalue Decomposition: PCA then performs an eigendecomposition of the covariance matrix to find the eigenvectors and eigenvalues. The eigenvectors represent the directions (principal components) along which the data varies the most, while the eigenvalues indicate the amount of variance along each eigenvector.\n",
    "\n",
    "Choosing Principal Components: The eigenvectors are ranked based on their corresponding eigenvalues, which represent the amount of variance they capture. The eigenvectors with the highest eigenvalues are selected as the principal components. These principal components form a new coordinate system in which the data is projected.\n",
    "\n",
    "Dimensionality Reduction: To perform dimensionality reduction, the data is projected onto the selected principal components. The original high-dimensional data can be approximated using a reduced number of principal components, which retains the most significant information in the data.\n",
    "\n",
    "Example:\n",
    "Consider a dataset of 2D points representing the heights and weights of individuals:\n",
    "| Height (cm) | Weight (kg) |\n",
    "|-------------|-------------|\n",
    "|    160      |    55       |\n",
    "|    170      |    70       |\n",
    "|    155      |    50       |\n",
    "|    175      |    65       |\n",
    "Covariance Matrix Calculation: Calculate the covariance matrix of the dataset.\n",
    "\n",
    "Eigenvector and Eigenvalue Decomposition: Perform eigendecomposition of the covariance matrix to find the principal components.\n",
    "\n",
    "Choosing Principal Components: Assume the first principal component explains 90% of the variance, and the second explains the remaining 10%.\n",
    "\n",
    "Dimensionality Reduction: Project the data onto the first principal component to reduce dimensionality while preserving the most significant variance.\n",
    "\n",
    "The resulting reduced-dimensional dataset might look like:\n",
    "| Principal Component 1 |\n",
    "|-----------------------|\n",
    "|        -1.15          |\n",
    "|         0.22          |\n",
    "|        -2.12          |\n",
    "|         3.05          |\n",
    "In this example, PCA has transformed the original height-weight data into a new coordinate system along the principal component with the most significant variance. The second principal component, which captures less variance, has been omitted, leading to dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a137b7a2-1346-406d-a355-6dddfa0bd7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba1984-5827-475a-a908-e147b67db89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA (Principal Component Analysis) is closely related to feature extraction and can be used as a feature extraction technique. Feature extraction involves transforming the original set of features into a new set of features that captures the most relevant information while reducing the dimensionality of the data. PCA is a dimensionality reduction technique that accomplishes this by identifying and extracting the most significant patterns or variations in the data.\n",
    "\n",
    "Here's how PCA can be used for feature extraction:\n",
    "\n",
    "Calculate the Covariance Matrix: Given a dataset with multiple features, compute the covariance matrix of the features.\n",
    "\n",
    "Perform Eigendecomposition: Find the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions of maximum variance (principal components), and the eigenvalues indicate the amount of variance along each eigenvector.\n",
    "\n",
    "Select Principal Components: Rank the eigenvectors based on their corresponding eigenvalues. Choose a subset of the top eigenvectors (principal components) that capture the desired amount of variance in the data.\n",
    "\n",
    "Project Data onto Principal Components: Project the original data onto the selected principal components to create a new set of features. These new features are linear combinations of the original features and represent a compressed representation of the data.\n",
    "\n",
    "By using PCA for feature extraction, you can reduce the dimensionality of the data while retaining most of its important variability and patterns.\n",
    "\n",
    "Example:\n",
    "Let's consider a dataset of grayscale images of handwritten digits. Each image has a size of 28x28 pixels, resulting in a high-dimensional feature space of 784 dimensions. We want to extract a reduced set of features to represent these images more compactly while preserving the essential information.\n",
    "\n",
    "Calculate Covariance Matrix: Compute the covariance matrix of the pixel intensities across all images.\n",
    "\n",
    "Perform Eigendecomposition: Find the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "Select Principal Components: Rank the eigenvectors based on their eigenvalues. Assume we choose the top 50 eigenvectors, which collectively capture, let's say, 95% of the variance.\n",
    "\n",
    "Project Data onto Principal Components: Project each image onto the selected principal components to obtain a new feature vector with reduced dimensions.\n",
    "\n",
    "As a result, the original 28x28 pixel images have been transformed into a new set of 50 features, which represent a compressed version of the original data. These new features can be used for various tasks such as classification, clustering, or visualization. The extracted features have captured the essential patterns in the data while reducing the dimensionality, which can lead to improved computational efficiency and better generalization in machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a823dcc-7ce9-4a6f-932f-c09c8af59328",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the context of building a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess the data and bring the features to a common scale. This ensures that the features are comparable and avoids certain features dominating others due to their different scales. Here's how you would use Min-Max scaling for the given dataset with features like price, rating, and delivery time:\n",
    "\n",
    "1. **Understand the Data:**\n",
    "   Start by understanding the range and distribution of each feature in the dataset. For example:\n",
    "   - Price: Range of prices for food items.\n",
    "   - Rating: Ratings given by users, usually in a range (e.g., 1 to 5).\n",
    "   - Delivery Time: Time taken for delivery, possibly in minutes.\n",
    "\n",
    "2. **Calculate Min-Max Scaling:**\n",
    "   For each feature, apply the Min-Max scaling formula to bring the values within the desired range (usually between 0 and 1):\n",
    "   \n",
    "   For a feature **x**, the Min-Max scaled value **x_scaled** is calculated as:\n",
    "   ```\n",
    "   x_scaled = (x - min(x)) / (max(x) - min(x))\n",
    "   ```\n",
    "\n",
    "   Calculate the scaled values for each feature (price, rating, and delivery time) based on their respective ranges.\n",
    "\n",
    "3. **Apply Min-Max Scaling:**\n",
    "   Replace the original values of each feature with their corresponding Min-Max scaled values. Now, all the features will be within the same range of 0 to 1.\n",
    "\n",
    "4. **Use the Preprocessed Data:**\n",
    "   The dataset with Min-Max scaled features can now be used as input for building the recommendation system. The scaled features are more suitable for various algorithms and analyses since they are normalized and don't have widely varying scales.\n",
    "\n",
    "Example:\n",
    "Let's consider a simplified example with a few data points for illustration:\n",
    "\n",
    "Original data:\n",
    "```\n",
    "|  Price  |  Rating  | Delivery Time |\n",
    "|---------|----------|---------------|\n",
    "|   15    |    4.5   |      30       |\n",
    "|   25    |    3.8   |      45       |\n",
    "|   10    |    4.2   |      20       |\n",
    "```\n",
    "\n",
    "1. **Understand the Data:** Price ranges from 10 to 25, rating ranges from 3.8 to 4.5, and delivery time ranges from 20 to 45.\n",
    "\n",
    "2. **Calculate Min-Max Scaling:**\n",
    "   - Price: (15 - 10) / (25 - 10) = 0.5\n",
    "   - Rating: (4.5 - 3.8) / (4.5 - 3.8) = 1.0\n",
    "   - Delivery Time: (30 - 20) / (45 - 20) = 0.625\n",
    "\n",
    "3. **Apply Min-Max Scaling:**\n",
    "Replace the original values with the scaled values:\n",
    "```\n",
    "|   Price  |  Rating  | Delivery Time |\n",
    "|----------|----------|---------------|\n",
    "|   0.5    |   1.0    |     0.625     |\n",
    "|   1.0    |   0.0    |     1.0       |\n",
    "|   0.0    |   0.6    |     0.0       |\n",
    "```\n",
    "\n",
    "4. **Use the Preprocessed Data:**\n",
    "The Min-Max scaled dataset can be used as input for training a recommendation system algorithm.\n",
    "\n",
    "By applying Min-Max scaling, the features are now within the range of 0 to 1, making them suitable for building a recommendation system that takes into account price, rating, and delivery time fairly and without biases due to different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374722d2-6324-4f71-af36-f011871da76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cdd9e7-7cbe-4f6c-978f-ac43cff190af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Using Principal Component Analysis (PCA) to reduce the dimensionality of a dataset when building a model to predict stock prices can help improve model performance, reduce overfitting, and enhance interpretability. Here's how you could use PCA in this context:\n",
    "\n",
    "1. **Data Preprocessing:**\n",
    "   Start by preparing your dataset, including cleaning missing data and normalizing the features if necessary. Ensure that your features are numerical and are on similar scales.\n",
    "\n",
    "2. **Feature Standardization:**\n",
    "   Before applying PCA, it's important to standardize the features so that they have a mean of 0 and a standard deviation of 1. This step is crucial for PCA to work effectively, as it's sensitive to the scale of features.\n",
    "\n",
    "3. **Apply PCA:**\n",
    "   Now you can apply PCA to the standardized dataset:\n",
    "   - Calculate the covariance matrix of the features.\n",
    "   - Perform eigendecomposition of the covariance matrix to obtain the eigenvectors and eigenvalues.\n",
    "   - Sort the eigenvectors in descending order based on their corresponding eigenvalues. These eigenvectors represent the principal components.\n",
    "   - Choose the top principal components that capture a significant portion of the variance in the data. You can decide on the number of components based on a desired level of variance explained (e.g., 95% or 99%).\n",
    "   - Project the original data onto the selected principal components to obtain the reduced-dimensional dataset.\n",
    "\n",
    "4. **Model Building and Evaluation:**\n",
    "   Use the reduced-dimensional dataset obtained from PCA for training your stock price prediction model. This reduced dataset can potentially improve model performance and generalization by reducing the curse of dimensionality and multicollinearity.\n",
    "\n",
    "Example:\n",
    "Suppose your original dataset contains financial data like revenue, profit, debt-to-equity ratio, and market trends like trading volume, volatility index, and interest rates. These features could be high-dimensional and potentially correlated.\n",
    "\n",
    "1. **Data Preprocessing:**\n",
    "   Clean the data, handle missing values, and ensure numerical features are on similar scales.\n",
    "\n",
    "2. **Feature Standardization:**\n",
    "   Standardize the features to have mean 0 and standard deviation 1.\n",
    "\n",
    "3. **Apply PCA:**\n",
    "   - Calculate the covariance matrix.\n",
    "   - Perform eigendecomposition.\n",
    "   - Assume you find that the top three eigenvectors explain 85% of the total variance.\n",
    "\n",
    "4. **Model Building and Evaluation:**\n",
    "   Use the three principal components as features to train your stock price prediction model, such as regression or a time series model. This reduced-dimensional dataset may lead to a more efficient and interpretable model, as well as reduced overfitting.\n",
    "\n",
    "By applying PCA in this manner, you'll effectively reduce the dimensionality of the dataset while retaining most of the essential information needed for predicting stock prices. This can lead to improved model performance and more efficient computational processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2950d5-0478-4315-bd29-0036763ccba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84fb46-e3a1-4b5c-aeb8-03e21bfbeb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "To perform Min-Max scaling on the given dataset and transform the values to a range of -1 to 1, you can use the following formula:\n",
    "\n",
    "```\n",
    "x_scaled = ((x - min(x)) / (max(x) - min(x))) * 2 - 1\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `x` is the original value.\n",
    "- `min(x)` is the minimum value in the dataset.\n",
    "- `max(x)` is the maximum value in the dataset.\n",
    "\n",
    "Let's apply this formula to the dataset [1, 5, 10, 15, 20]:\n",
    "\n",
    "1. Find the minimum and maximum values:\n",
    "   - `min(x) = 1`\n",
    "   - `max(x) = 20`\n",
    "\n",
    "2. Apply the Min-Max scaling formula to each value in the dataset:\n",
    "\n",
    "   For `x = 1`:\n",
    "   ```\n",
    "   x_scaled = ((1 - 1) / (20 - 1)) * 2 - 1 = 0\n",
    "   ```\n",
    "\n",
    "   For `x = 5`:\n",
    "   ```\n",
    "   x_scaled = ((5 - 1) / (20 - 1)) * 2 - 1 = -0.6\n",
    "   ```\n",
    "\n",
    "   For `x = 10`:\n",
    "   ```\n",
    "   x_scaled = ((10 - 1) / (20 - 1)) * 2 - 1 = -0.2\n",
    "   ```\n",
    "\n",
    "   For `x = 15`:\n",
    "   ```\n",
    "   x_scaled = ((15 - 1) / (20 - 1)) * 2 - 1 = 0.2\n",
    "   ```\n",
    "\n",
    "   For `x = 20`:\n",
    "   ```\n",
    "   x_scaled = ((20 - 1) / (20 - 1)) * 2 - 1 = 1\n",
    "   ```\n",
    "\n",
    "The Min-Max scaled values for the dataset [1, 5, 10, 15, 20] in the range of -1 to 1 are approximately:\n",
    "\n",
    "```\n",
    "[-1.0, -0.6, -0.2, 0.2, 1.0]\n",
    "```\n",
    "\n",
    "These scaled values ensure that the original data is transformed to lie within the specified range while preserving their relative differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9ed88-807d-4442-a9ae-487a9a3a8fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47d45f8-555a-4a4d-a6e6-dd9b2feade6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Choosing the number of principal components to retain in PCA depends on various factors, including the desired level of explained variance, the specific application, and the trade-off between dimensionality reduction and preserving information. To decide on the number of principal components to retain, you can follow these steps:\n",
    "\n",
    "1. **Calculate the Covariance Matrix:** Compute the covariance matrix of the features [height, weight, age, gender, blood pressure].\n",
    "\n",
    "2. **Perform Eigendecomposition:** Find the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "3. **Determine Explained Variance:** Examine the eigenvalues to understand the amount of variance captured by each principal component. The explained variance is a crucial factor in deciding how many principal components to retain. You can calculate the explained variance ratio for each principal component:\n",
    "\n",
    "   ```\n",
    "   Explained Variance Ratio = (Eigenvalue of Principal Component) / (Sum of all Eigenvalues)\n",
    "   ```\n",
    "\n",
    "4. **Cumulative Explained Variance:** Calculate the cumulative explained variance by summing the explained variance ratios of the principal components in descending order. This will give you an idea of how much total variance is retained as you add more principal components.\n",
    "\n",
    "5. **Choose the Number of Principal Components:** Decide on the number of principal components to retain based on the cumulative explained variance and your application requirements. Common approaches include selecting a number of components that retain a certain percentage of the total variance (e.g., 95% or 99%).\n",
    "\n",
    "6. **Project Data onto Selected Principal Components:** Project the original data onto the selected principal components to obtain the reduced-dimensional dataset.\n",
    "\n",
    "In terms of gender, which is a categorical feature, it might need to be one-hot encoded before applying PCA. Alternatively, you could consider using other techniques like binary encoding.\n",
    "\n",
    "The choice of the number of principal components to retain is subjective and depends on your specific goals and constraints. Retaining fewer components can lead to more compact representations with reduced dimensionality, but it may also result in some loss of information. Retaining more components can capture more variance but may lead to increased complexity.\n",
    "\n",
    "For example, let's say you find that the first two principal components capture 85% of the total variance. In this case, you might choose to retain these two principal components to balance dimensionality reduction and information preservation.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
