{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7cad832-01b4-42d0-9beb-9126b1da9981",
   "metadata": {},
   "source": [
    "## Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d83e46-fc41-4b07-8647-b8adb47af6f8",
   "metadata": {},
   "source": [
    "Lasso Regression, or Least Absolute Shrinkage and Selection Operator, is a linear regression technique that incorporates a regularization term to the ordinary least squares (OLS) regression loss function. Lasso Regression helps address the issue of multicollinearity and performs automatic variable selection by enforcing some of the coefficients to be exactly zero. This makes Lasso Regression useful for feature selection in high-dimensional datasets.\n",
    "\n",
    "Here are key characteristics and differences of Lasso Regression compared to other regression techniques:\n",
    "\n",
    "1. **Regularization Term:**\n",
    "   - Lasso Regression adds a regularization term to the OLS loss function, which is proportional to the absolute values of the regression coefficients. The regularization term is controlled by a tuning parameter (\\(\\lambda\\)), and it penalizes large coefficients. The inclusion of the absolute values of coefficients promotes sparsity by encouraging some coefficients to be exactly zero.\n",
    "\n",
    "2. **Sparsity and Feature Selection:**\n",
    "   - Lasso Regression promotes sparsity in the model, meaning that it tends to produce models with a smaller number of non-zero coefficients. This inherent feature makes Lasso Regression particularly suitable for feature selection in situations where only a subset of predictors is expected to be relevant.\n",
    "\n",
    "3. **Mathematical Formulation:**\n",
    "   - The Lasso Regression loss function is given by:\n",
    "     loss function = linear regression loss function(MSE)+ lambda [(sum of i=0 to n)beta_j ]\n",
    "\n",
    "4. **Difference from Ridge Regression:**\n",
    "   - Lasso Regression differs from Ridge Regression in terms of the regularization term. While Ridge Regression uses the sum of squared coefficients in its regularization term, Lasso Regression uses the sum of absolute values of coefficients. As a result, Ridge Regression tends to shrink coefficients towards zero, but rarely exactly to zero, while Lasso Regression can set some coefficients to exactly zero.\n",
    "\n",
    "5. **Handling Multicollinearity:**\n",
    "   - Like Ridge Regression, Lasso Regression is effective in handling multicollinearity. However, Lasso Regression's ability to perform variable selection makes it particularly attractive when dealing with a large number of predictors where many may be irrelevant.\n",
    "\n",
    "6. **Impact on Coefficient Magnitudes:**\n",
    "   - Due to the regularization term, Lasso Regression tends to produce coefficient estimates that are smaller in magnitude compared to OLS regression. The strength of the regularization is controlled by the tuning parameter (\\(\\lambda\\)), and larger values of \\(\\lambda\\) result in stronger regularization.\n",
    "\n",
    "In summary, Lasso Regression is a regression technique that incorporates a regularization term to perform both variable selection and regularization. Its ability to set some coefficients exactly to zero makes it useful for feature selection, especially in situations with high-dimensional datasets where not all predictors are expected to be relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8888e-13f4-43c3-831c-6932c694ec9d",
   "metadata": {},
   "source": [
    "## Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a401655a-e447-430b-9d79-5736420fc0c9",
   "metadata": {},
   "source": [
    "The main advantage of using Lasso Regression in feature selection lies in its ability to automatically shrink the coefficients of irrelevant or less important predictors exactly to zero. This property makes Lasso Regression particularly effective in situations where the dataset has a large number of predictors, and only a subset of them is expected to be relevant for predicting the target variable. Here are the key advantages:\n",
    "\n",
    "1. **Automatic Variable Selection:**\n",
    "   - Lasso Regression performs automatic variable selection by setting the coefficients of some predictors to exactly zero. This means that Lasso naturally identifies and excludes irrelevant predictors from the model. The resulting sparse model is simpler and easier to interpret.\n",
    "\n",
    "2. **Sparse Models:**\n",
    "   - Lasso tends to produce sparse models with a smaller subset of predictors having non-zero coefficients. In cases where there are many predictors, but only a few are truly important, Lasso's sparsity property is advantageous for creating a more interpretable and computationally efficient model.\n",
    "\n",
    "3. **Reduced Overfitting:**\n",
    "   - By excluding irrelevant predictors, Lasso helps reduce overfitting, which occurs when a model captures noise in the training data rather than the underlying patterns. The regularization term in Lasso helps prevent the model from becoming too complex, improving its generalization performance to new, unseen data.\n",
    "\n",
    "4. **Handling Multicollinearity:**\n",
    "   - Like Ridge Regression, Lasso is effective in handling multicollinearity, but its ability to set coefficients exactly to zero provides an additional benefit. In the presence of highly correlated predictors, Lasso automatically chooses one of the correlated predictors and sets the coefficients of the others to zero.\n",
    "\n",
    "5. **Feature Subset Selection:**\n",
    "   - Lasso allows practitioners to directly obtain a subset of the most important features, making it a valuable tool for situations where there is a desire to identify and focus on a limited set of predictors.\n",
    "\n",
    "6. **Improves Model Interpretability:**\n",
    "   - Lasso's ability to create sparse models with a reduced number of predictors enhances the interpretability of the model. It simplifies the understanding of the relationships between predictors and the target variable.\n",
    "\n",
    "It's important to note that while Lasso Regression is powerful for feature selection, the choice of the regularization parameter (\\(\\lambda\\)) is crucial. Cross-validation or other model selection techniques are often used to find an optimal \\(\\lambda\\) that balances the trade-off between fitting the data well and sparsity in the model. Additionally, when predictor variables are highly correlated, Lasso may arbitrarily choose one of them and set the others to zero, which may be a consideration in the interpretation of results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3774c81-af6a-41e9-ae81-c70130331740",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3307e633-97ad-4401-860e-7e4aac3b4091",
   "metadata": {},
   "source": [
    "Interpreting the coefficients of a Lasso Regression model involves considerations that differ from traditional linear regression due to the sparsity-inducing property of Lasso. Here's a guide on interpreting the coefficients in a Lasso Regression model:\n",
    "\n",
    "1. **Magnitude and Sign:**\n",
    "   - As in linear regression, the sign of a coefficient in Lasso Regression indicates the direction of the relationship between the predictor variable and the response variable. A positive coefficient suggests a positive relationship, while a negative coefficient suggests a negative relationship.\n",
    "\n",
    "   - The magnitude of the coefficients in Lasso Regression is influenced by the regularization term, and they may be smaller in magnitude compared to ordinary least squares (OLS) regression. The regularization term shrinks the coefficients towards zero, especially for less important predictors.\n",
    "\n",
    "2. **Zero Coefficients:**\n",
    "   - One of the key features of Lasso Regression is its ability to set some coefficients exactly to zero. A coefficient being exactly zero means that the corresponding predictor has been excluded from the model. This property allows Lasso Regression to perform automatic variable selection.\n",
    "\n",
    "3. **Non-Zero Coefficients:**\n",
    "   - Non-zero coefficients indicate that the corresponding predictors have been deemed important by the Lasso Regression model. The larger the absolute value of a non-zero coefficient, the more impact that predictor has on the response variable.\n",
    "\n",
    "4. **Interaction Terms:**\n",
    "   - If interaction terms are included in the Lasso Regression model, the interpretation is similar to that in linear regression. Interaction terms represent the combined effect of the respective predictors on the response variable.\n",
    "\n",
    "5. **Relative Importance:**\n",
    "   - The relative importance of predictors can be inferred from the magnitudes of the non-zero coefficients. Features with larger absolute values of coefficients have a greater impact on the predictions.\n",
    "\n",
    "6. **Scaling Matters:**\n",
    "   - Lasso Regression is sensitive to the scale of predictor variables. It is common practice to standardize or normalize the variables before applying Lasso Regression. If scaling is performed, the coefficients represent the change in the response variable per standard deviation change in the predictor variable.\n",
    "\n",
    "7. **Compare with OLS:**\n",
    "   - For comparison purposes, analysts may consider running an ordinary least squares (OLS) regression on the same data and comparing the coefficients. The OLS coefficients can provide a reference point for interpretation, as they are not subject to regularization.\n",
    "\n",
    "8. **Choice of Regularization Parameter:**\n",
    "   - The strength of the regularization in Lasso Regression is controlled by the tuning parameter (\\(\\lambda\\)). A larger \\(\\lambda\\) results in stronger regularization and more coefficients being set to zero. The choice of \\(\\lambda\\) is critical and is often determined through cross-validation or other model selection techniques.\n",
    "\n",
    "In summary, interpreting coefficients in Lasso Regression involves considering the sparsity of the model and understanding the impact of the regularization term. The zero/non-zero nature of coefficients provides direct information about variable importance, making Lasso Regression a valuable tool for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9b207a-3039-42f1-88c3-ca385bef8f1b",
   "metadata": {},
   "source": [
    "## Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5b76fd-bf91-4e95-a76b-d93f83b890b9",
   "metadata": {},
   "source": [
    "In Lasso Regression, the primary tuning parameter is denoted by \\(\\lambda\\) (lambda), and it controls the strength of the regularization applied to the model. The tuning parameter \\(\\lambda\\) is crucial in determining the balance between fitting the data well and inducing sparsity in the model. The higher the value of \\(\\lambda\\), the stronger the regularization, leading to more coefficients being pushed toward zero.\n",
    "\n",
    "Here are the key tuning parameters in Lasso Regression and their impact on the model's performance:\n",
    "\n",
    "1. **Regularization Parameter (\\(\\lambda\\)):**\n",
    "   - The main tuning parameter in Lasso Regression is \\(\\lambda\\), which controls the strength of the penalty applied to the sum of absolute values of coefficients. A larger \\(\\lambda\\) leads to stronger regularization, favoring sparsity by setting more coefficients exactly to zero.\n",
    "\n",
    "2. **Model Complexity:**\n",
    "   - The choice of \\(\\lambda\\) affects the complexity of the Lasso model. Smaller values of \\(\\lambda\\) result in less regularization, allowing the model to be more complex and potentially overfit the training data. Larger values of \\(\\lambda\\) lead to a simpler, more regularized model.\n",
    "\n",
    "3. **Bias-Variance Trade-Off:**\n",
    "   - Adjusting \\(\\lambda\\) involves a bias-variance trade-off. A smaller \\(\\lambda\\) reduces bias but increases variance, making the model more prone to overfitting. Conversely, a larger \\(\\lambda\\) increases bias but reduces variance, making the model more robust to noise and potentially improving its generalization to new data.\n",
    "\n",
    "4. **Cross-Validation:**\n",
    "   - Cross-validation, such as k-fold cross-validation, is commonly used to find the optimal value of \\(\\lambda\\) by assessing the model's performance on different subsets of the data. This helps strike a balance between model complexity and goodness of fit.\n",
    "\n",
    "5. **Path of Regularization:**\n",
    "   - In practice, many implementations of Lasso Regression provide a regularization path, which shows how the coefficients change for different values of \\(\\lambda\\). Analysts can examine this path to understand the impact of the regularization parameter on individual coefficients.\n",
    "\n",
    "6. **Model Stability:**\n",
    "   - A well-chosen \\(\\lambda\\) can enhance the stability of the Lasso model by preventing overfitting and making the model less sensitive to small changes in the training data.\n",
    "\n",
    "7. **Feature Selection:**\n",
    "   - The tuning parameter \\(\\lambda\\) directly influences the extent of feature selection performed by Lasso Regression. As \\(\\lambda\\) increases, more coefficients are set to zero, leading to a sparser model.\n",
    "\n",
    "Choosing the appropriate value of \\(\\lambda\\) is crucial for the effective application of Lasso Regression. Cross-validation is a common method for tuning \\(\\lambda\\), where the model's performance is evaluated on different subsets of the data for various values of \\(\\lambda\\), and the one that optimizes a chosen criterion (e.g., mean squared error, cross-validated error) is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8352a3a-317e-4c66-97c4-a16e658c6958",
   "metadata": {},
   "source": [
    "## Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b29d83-42b8-4b82-adda-a10534f6fa1a",
   "metadata": {},
   "source": [
    "Lasso Regression, in its standard form, is a linear regression technique designed for linear relationships between predictors and the response variable. It applies regularization to the linear regression model to handle multicollinearity and perform automatic variable selection. However, it is inherently a linear method and does not handle non-linear relationships between predictors and the response variable.\n",
    "\n",
    "If you're dealing with non-linear regression problems, where the relationship between predictors and the response variable is not linear, standard Lasso Regression may not be the most appropriate choice. In such cases, you may consider the following alternatives:\n",
    "\n",
    "1. **Polynomial Features:**\n",
    "   - One way to extend linear models, including Lasso Regression, to capture non-linear relationships is by creating polynomial features. You can introduce higher-order terms of existing predictors (e.g., quadratic or cubic terms) to allow the model to capture non-linear patterns. However, be cautious about overfitting, especially with a large number of polynomial features.\n",
    "\n",
    "2. **Kernelized Methods:**\n",
    "   - Kernelized methods, such as Support Vector Machines (SVM) with non-linear kernels or kernelized Ridge Regression, can be used to model non-linear relationships. These methods implicitly map the input features into a higher-dimensional space, allowing them to capture complex relationships.\n",
    "\n",
    "3. **Decision Trees and Random Forests:**\n",
    "   - Decision trees and ensemble methods like Random Forests are capable of capturing non-linear patterns in the data. They are particularly useful when dealing with complex relationships and interactions between variables.\n",
    "\n",
    "4. **Generalized Additive Models (GAM):**\n",
    "   - GAMs are an extension of linear models that allow for non-linear relationships between predictors and the response variable. They use spline functions to model non-linearities, and regularization terms can be included to control the complexity of the model.\n",
    "\n",
    "5. **Neural Networks:**\n",
    "   - Deep learning techniques, such as neural networks, are powerful tools for capturing complex non-linear relationships. Neural networks can automatically learn hierarchical representations of data, making them well-suited for non-linear regression problems.\n",
    "\n",
    "6. **Non-linear Regression Models:**\n",
    "   - Specific non-linear regression models, like exponential growth models, logistic regression, or other non-linear forms, may be more appropriate for certain types of relationships. Choosing a model that explicitly represents the expected non-linear pattern in the data can improve performance.\n",
    "\n",
    "When selecting a method for non-linear regression, it's essential to understand the underlying patterns in the data and choose an approach that aligns with the characteristics of the problem. Each of the mentioned methods has its strengths and weaknesses, and the choice depends on factors such as interpretability, complexity, and the amount of available data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58092c72-243c-4d14-b763-b979c17adad2",
   "metadata": {},
   "source": [
    "## Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6113161-e8ed-407d-835e-69cc485d8ee9",
   "metadata": {},
   "source": [
    "Ridge Regression and Lasso Regression are both techniques for linear regression with regularization, but they differ in the type of regularization and the impact on the regression coefficients. Here are the key differences between Ridge and Lasso Regression:\n",
    "\n",
    "1. **Regularization Term:**\n",
    "   - **Ridge Regression:** The regularization term in Ridge Regression is the sum of squared values of the coefficients, multiplied by a tuning parameter \\(\\lambda\\). The regularization term is \\(\\lambda \\sum_{j=1}^{p}\\beta_j^2\\), where \\(\\beta_j\\) are the regression coefficients.\n",
    "   \n",
    "   - **Lasso Regression:** The regularization term in Lasso Regression is the sum of absolute values of the coefficients, multiplied by a tuning parameter \\(\\lambda\\). The regularization term is \\(\\lambda \\sum_{j=1}^{p}|\\beta_j|\\), where \\(\\beta_j\\) are the regression coefficients.\n",
    "\n",
    "2. **Sparsity:**\n",
    "   - **Ridge Regression:** Ridge Regression tends to shrink the coefficients towards zero but rarely sets them exactly to zero. It does not perform variable selection in the sense that it includes all predictors in the model, even if with very small coefficients.\n",
    "   \n",
    "   - **Lasso Regression:** Lasso Regression, due to its use of the absolute values in the regularization term, has the ability to set some coefficients exactly to zero. This leads to sparsity in the model, effectively performing automatic variable selection by excluding some predictors from the model.\n",
    "\n",
    "3. **Impact on Coefficients:**\n",
    "   - **Ridge Regression:** The regularization term in Ridge Regression penalizes the squared magnitude of coefficients, leading to a gradual shrinking of all coefficients, but rarely setting them exactly to zero.\n",
    "   \n",
    "   - **Lasso Regression:** The regularization term in Lasso Regression penalizes the absolute magnitude of coefficients, leading to some coefficients being exactly zero. This encourages sparsity and results in a model with fewer predictors.\n",
    "\n",
    "4. **Handling Multicollinearity:**\n",
    "   - **Ridge Regression:** Ridge Regression is effective in handling multicollinearity by shrinking correlated coefficients towards each other. It does not, however, perform variable selection and retains all predictors in the model.\n",
    "   \n",
    "   - **Lasso Regression:** Lasso Regression handles multicollinearity similarly to Ridge Regression but also has the additional benefit of feature selection. In the presence of highly correlated predictors, Lasso may choose one and set the coefficients of others to zero.\n",
    "\n",
    "5. **Optimization Objective:**\n",
    "   - **Ridge Regression:** The optimization objective in Ridge Regression is to minimize the sum of squared errors (ordinary least squares) while penalizing large coefficients.\n",
    "   \n",
    "   - **Lasso Regression:** The optimization objective in Lasso Regression is to minimize the sum of squared errors with the additional penalty for the absolute values of coefficients.\n",
    "\n",
    "In summary, while both Ridge and Lasso Regression introduce regularization to address multicollinearity, Ridge tends to shrink coefficients towards zero without setting them exactly to zero, while Lasso can lead to sparsity by setting some coefficients exactly to zero, performing automatic variable selection. The choice between Ridge and Lasso often depends on the specific characteristics of the dataset and the desired properties of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268a828-44ed-4fa0-af1a-09f77bc7f9a8",
   "metadata": {},
   "source": [
    " ## Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4eb0f1-0d92-4bc1-bffb-cdf9514891b7",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can handle multicollinearity in the input features, and it does so by incorporating a regularization term that penalizes the absolute values of the regression coefficients. Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, making it challenging to separate their individual effects on the dependent variable. Lasso Regression's regularization term addresses this issue in the following ways:\n",
    "\n",
    "1. **Shrinkage of Coefficients:**\n",
    "   - The regularization term in Lasso Regression penalizes the sum of the absolute values of the regression coefficients. As a result, during the optimization process, Lasso tends to shrink some coefficients exactly to zero, effectively eliminating the corresponding predictors from the model.\n",
    "\n",
    "2. **Variable Selection:**\n",
    "   - Lasso Regression's ability to set some coefficients exactly to zero makes it particularly useful for automatic variable selection. When faced with multicollinearity, Lasso can choose one predictor and set the coefficients of the other correlated predictors to zero. This feature helps in identifying and prioritizing the most important predictors in the presence of redundancy.\n",
    "\n",
    "3. **Sparsity Inducing:**\n",
    "   - The sparsity-inducing property of Lasso Regression is beneficial in dealing with multicollinearity. Sparsity implies that the model has fewer non-zero coefficients, leading to a simpler and more interpretable model. In the context of multicollinearity, sparsity allows Lasso to focus on a subset of predictors that are most informative.\n",
    "\n",
    "4. **Trade-Off Between Variables:**\n",
    "   - Lasso Regression introduces a trade-off between fitting the data well and keeping the model simple. As the regularization parameter (\\(\\lambda\\)) increases, the penalty on the absolute values of coefficients becomes stronger, leading to more coefficients being set to zero. This trade-off allows Lasso to handle multicollinearity by favoring simpler models with fewer predictors.\n",
    "\n",
    "It's important to note that while Lasso Regression is effective in handling multicollinearity, the choice of the regularization parameter (\\(\\lambda\\)) is crucial. The analyst needs to select an appropriate \\(\\lambda\\) value that balances the need for sparsity with the goal of fitting the data well. Cross-validation or other model selection techniques are often used to find the optimal \\(\\lambda\\) value for a given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b051d21-68eb-41c0-91c9-9be25022a54c",
   "metadata": {},
   "source": [
    "## Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99694b4-285f-4c22-8de7-b38fbb9a3bb9",
   "metadata": {},
   "source": [
    "Choosing the optimal value of the regularization parameter (\\(\\lambda\\)) in Lasso Regression is a critical step to balance model complexity and goodness of fit. There are several methods to determine the optimal \\(\\lambda\\), and cross-validation is a commonly used technique. Here's a step-by-step guide:\n",
    "\n",
    "1. **Grid Search:**\n",
    "   - Start by defining a range of \\(\\lambda\\) values to explore. This range should cover a spectrum from very small values (little to no regularization) to large values (strong regularization). A common approach is to use a logarithmic scale for the \\(\\lambda\\) values.\n",
    "\n",
    "2. **Cross-Validation:**\n",
    "   - Divide your dataset into training and validation sets. Implement k-fold cross-validation, where the training set is further split into k subsets. Train the Lasso Regression model on k-1 folds and validate it on the remaining fold, repeating this process k times, each time using a different fold as the validation set.\n",
    "\n",
    "3. **Model Evaluation:**\n",
    "   - For each \\(\\lambda\\) value, calculate the average performance metric (e.g., mean squared error, R-squared) across all folds. This provides a more robust estimate of how well the model generalizes to unseen data.\n",
    "\n",
    "4. **Select Optimal \\(\\lambda\\):**\n",
    "   - Choose the \\(\\lambda\\) that yields the best performance on the validation set. This is typically the \\(\\lambda\\) with the lowest mean squared error or the highest R-squared value.\n",
    "\n",
    "5. **Final Model Training:**\n",
    "   - Once the optimal \\(\\lambda\\) is identified, train the Lasso Regression model on the entire training dataset using this selected \\(\\lambda\\).\n",
    "\n",
    "6. **Test Set Evaluation:**\n",
    "   - Assess the final model's performance on a separate test set to ensure an unbiased evaluation of its generalization ability.\n",
    "\n",
    "Common cross-validation techniques include k-fold cross-validation (where \\(k\\) is a user-defined number, often 5 or 10) and leave-one-out cross-validation (LOOCV), where each data point serves as a validation set exactly once.\n",
    "\n",
    "Some programming environments and libraries, such as scikit-learn in Python, provide functions for automating this process. For instance, scikit-learn's `LassoCV` class performs cross-validated Lasso Regression and automatically selects the optimal \\(\\lambda\\) based on cross-validation performance.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Specify a range of alphas (lambdas)\n",
    "alphas = [0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "# Create a LassoCV model with k-fold cross-validation\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=KFold(n_splits=5, shuffle=True, random_state=42))\n",
    "\n",
    "# Fit the model to the data\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "# Optimal alpha (lambda)\n",
    "optimal_alpha = lasso_cv.alpha_\n",
    "```\n",
    "\n",
    "Adjust the range of \\(\\lambda\\) values and the choice of cross-validation technique based on the characteristics of your data. It's essential to strike a balance between model complexity and generalization performance when selecting the optimal \\(\\lambda\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16fbe2f7-10f2-42a4-8f02-d31e799e7fe2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m lasso_cv \u001b[38;5;241m=\u001b[39m LassoCV(alphas\u001b[38;5;241m=\u001b[39malphas, cv\u001b[38;5;241m=\u001b[39mKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Fit the model to the data\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m lasso_cv\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Optimal alpha (lambda)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m optimal_alpha \u001b[38;5;241m=\u001b[39m lasso_cv\u001b[38;5;241m.\u001b[39malpha_\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Specify a range of alphas (lambdas)\n",
    "alphas = [0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "# Create a LassoCV model with k-fold cross-validation\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=KFold(n_splits=5, shuffle=True, random_state=42))\n",
    "\n",
    "# Fit the model to the data\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "# Optimal alpha (lambda)\n",
    "optimal_alpha = lasso_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ae6283-2a0c-482f-bacc-bec52acc81c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
