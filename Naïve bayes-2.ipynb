{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59503db5-f2bd-4dfa-92e3-66e4f9f0a6e1",
   "metadata": {},
   "source": [
    "## Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917baa2-5ac1-48e7-88cb-843a13b004d9",
   "metadata": {},
   "source": [
    "The probability that an employee is a smoker given that he/she uses the health insurance plan can be calculated using the conditional probability formula. \n",
    "\n",
    "Let's denote:\n",
    "- \\( A \\): The event that an employee uses the health insurance plan.\n",
    "- \\( B \\): The event that an employee is a smoker.\n",
    "\n",
    "The probability of an employee using the health insurance plan is \\( P(A) = 0.70 \\) (70%), and the probability of an employee who uses the plan being a smoker is \\( P(B|A) = 0.40 \\) (40%).\n",
    "\n",
    "The conditional probability of being a smoker given the use of the health insurance plan is given by:\n",
    "\n",
    "\\[ P(B|A) = \\frac{P(A \\cap B)}{P(A)} \\]\n",
    "\n",
    "where \\( P(A \\cap B) \\) is the probability of both events A and B occurring.\n",
    "\n",
    "We know that \\( P(B|A) = 0.40 \\) and \\( P(A) = 0.70 \\). Rearranging the formula to solve for \\( P(A \\cap B) \\), we get:\n",
    "\n",
    "\\[ P(A \\cap B) = P(B|A) \\cdot P(A) \\]\n",
    "\n",
    "Substitute the values:\n",
    "\n",
    "\\[ P(A \\cap B) = 0.40 \\cdot 0.70 \\]\n",
    "\n",
    "Now, calculate the result:\n",
    "\n",
    "\\[ P(A \\cap B) = 0.28 \\]\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is \\( 0.28 \\) or 28%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7f8cfc-e64b-4fb5-a1a2-3a03be612e63",
   "metadata": {},
   "source": [
    "## Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e2028f-0b5d-48d7-9bb1-cb3f8a7f8526",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes algorithm, and they differ in terms of the types of data they are designed to handle.\n",
    "\n",
    "1. **Bernoulli Naive Bayes:**\n",
    "   - **Features:** It is specifically designed for binary features, meaning features that can take on only two values (0 or 1).\n",
    "   - **Use Case:** It is often used in text classification problems, where the presence or absence of words in a document is considered. Each feature represents the presence (1) or absence (0) of a term.\n",
    "   - **Probability Model:** Assumes that features are binary variables and follows a Bernoulli distribution. The model considers only whether a particular feature is present or not, not the frequency of its occurrence.\n",
    "\n",
    "2. **Multinomial Naive Bayes:**\n",
    "   - **Features:** It is suitable for features that represent counts or frequencies. The features are non-negative integers, typically representing the number of occurrences of a term.\n",
    "   - **Use Case:** Commonly used in text classification problems, especially when the frequency of words matters. It considers the frequency of each term in the document.\n",
    "   - **Probability Model:** Assumes that features are generated from a multinomial distribution. It models the likelihood of observing a particular frequency of each term.\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "- **Type of Features:**\n",
    "  - Bernoulli Naive Bayes is suitable for binary features.\n",
    "  - Multinomial Naive Bayes is suitable for features representing counts or frequencies.\n",
    "\n",
    "- **Probability Distribution:**\n",
    "  - Bernoulli Naive Bayes assumes a Bernoulli distribution for features.\n",
    "  - Multinomial Naive Bayes assumes a multinomial distribution for features.\n",
    "\n",
    "- **Representation of Data:**\n",
    "  - In Bernoulli Naive Bayes, the focus is on the presence or absence of features (binary representation).\n",
    "  - In Multinomial Naive Bayes, the focus is on the frequencies or counts of features.\n",
    "\n",
    "- **Application:**\n",
    "  - Bernoulli Naive Bayes is often used in text classification problems where the presence or absence of words is important.\n",
    "  - Multinomial Naive Bayes is also used in text classification but takes into account the frequency of words.\n",
    "\n",
    "Both variants make the \"naive\" assumption of feature independence, meaning that features are considered to be conditionally independent given the class label. The choice between Bernoulli and Multinomial Naive Bayes depends on the nature of the data and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5533e887-190c-4b90-a4b8-4eefef60d389",
   "metadata": {},
   "source": [
    "## Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3c1496-b778-4e69-9787-7f5443446e35",
   "metadata": {},
   "source": [
    "In the context of Naive Bayes classifiers, including Bernoulli Naive Bayes, handling missing values is an important consideration. The presence of missing values in the dataset can affect the performance of the classifier, and various strategies can be employed to address this issue. Here are some common approaches:\n",
    "\n",
    "1. **Ignoring Missing Values:**\n",
    "   - One simple approach is to ignore instances with missing values during training and classification. This means that any instance with missing values in one or more features is excluded from consideration. While straightforward, this approach may lead to a loss of information.\n",
    "\n",
    "2. **Imputation:**\n",
    "   - Another strategy is to impute (fill in) missing values with some estimated values. Common imputation methods include replacing missing values with the mean, median, or mode of the observed values in the feature. However, for Bernoulli Naive Bayes, which deals with binary features, imputation might not be as straightforward.\n",
    "\n",
    "3. **Treating Missing as a Separate Category:**\n",
    "   - Instead of imputing missing values, you can treat missing values as a separate category or level for each feature. This approach is feasible for categorical or discrete features but may not be suitable for binary features in a Bernoulli Naive Bayes setting.\n",
    "\n",
    "4. **Advanced Imputation Techniques:**\n",
    "   - For more advanced scenarios, you may explore machine learning-based imputation techniques, such as using other classifiers or regression models to predict missing values based on the available information. This can be especially useful when dealing with complex relationships in the data.\n",
    "\n",
    "The specific choice of how to handle missing values depends on the characteristics of the data and the problem at hand. It's important to carefully consider the implications of each approach and evaluate their impact on the performance of the classifier.\n",
    "\n",
    "In the case of Bernoulli Naive Bayes, where features are binary, the handling of missing values should align with the nature of binary features. Ignoring missing values or treating them as a separate category are more straightforward options in this context. Imputation might require additional consideration and may not be as intuitive as in the case of continuous or categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb72af9-a528-4e82-94b8-823e7580b84e",
   "metadata": {},
   "source": [
    "## Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ccd2cc-f2f2-45d3-9f7f-99ff3d44dd61",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. The Gaussian Naive Bayes classifier is an extension of the Naive Bayes algorithm that is designed for continuous features, assuming that the values of each feature are normally (Gaussian) distributed within each class. Despite the \"Gaussian\" assumption, it can still be applied to problems with more than two classes.\n",
    "\n",
    "In the context of multi-class classification, the classifier assigns a class label to an instance based on the class that maximizes the posterior probability given the observed feature values. The decision rule for multi-class Gaussian Naive Bayes is typically based on comparing the posterior probabilities for each class.\n",
    "\n",
    "The probability of an instance belonging to class \\( C_i \\) given its feature vector \\( X \\) is calculated using Bayes' theorem:\n",
    "\n",
    "\\[ P(C_i | X) \\propto P(X | C_i) \\cdot P(C_i) \\]\n",
    "\n",
    "Here:\n",
    "- \\( P(C_i | X) \\) is the posterior probability of class \\( C_i \\) given the observed features.\n",
    "- \\( P(X | C_i) \\) is the likelihood of the observed features given class \\( C_i \\), which is modeled as a multivariate Gaussian distribution.\n",
    "- \\( P(C_i) \\) is the prior probability of class \\( C_i \\).\n",
    "\n",
    "The classification decision is made by selecting the class with the highest posterior probability.\n",
    "\n",
    "While Gaussian Naive Bayes is applicable to multi-class problems, it's important to note that its performance may be influenced by the assumption of feature independence, which is part of the \"naive\" aspect of the algorithm. Additionally, its effectiveness depends on the degree to which the feature distributions within each class approximate a multivariate Gaussian distribution. In practice, it is often used for problems where the Gaussian assumption is reasonable and computational efficiency is a priority."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149a9465-441b-4268-b9b9-27a481bc6a9c",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "Note: Create your assignment in Jupyter notebook and upload it to GitHub & share that github repository\n",
    "link through your dashboard. Make sure the repository is public.\n",
    "Note: This dataset contains a binary classification problem with multiple features. The dataset is\n",
    "relatively small, but it can be used to demonstrate the performance of the different variants of Naive\n",
    "Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ba34b6a-07ed-432c-9513-397dceaa8a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "columns = [...]  # Specify column names based on the dataset documentation\n",
    "data = pd.read_csv(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "007c8271-69bb-4954-8e5c-f3794b27bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Implement and evaluate Bernoulli Naive Bayes\n",
    "bernoulli_nb = BernoulliNB()\n",
    "bernoulli_scores = cross_val_score(bernoulli_nb, X, y, cv=10)\n",
    "\n",
    "# Implement and evaluate Multinomial Naive Bayes\n",
    "multinomial_nb = MultinomialNB()\n",
    "multinomial_scores = cross_val_score(multinomial_nb, X, y, cv=10)\n",
    "\n",
    "# Implement and evaluate Gaussian Naive Bayes\n",
    "gaussian_nb = GaussianNB()\n",
    "gaussian_scores = cross_val_score(gaussian_nb, X, y, cv=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5511c162-c0a0-4e98-8240-4fab2547d35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes Metrics: (0.8839130434782608, 0.8812649164677804, 0.815121412803532, 0.8469036697247706)\n",
      "Multinomial Naive Bayes Metrics: (0.7860869565217391, 0.7320627802690582, 0.7207505518763797, 0.7263626251390434)\n",
      "Gaussian Naive Bayes Metrics: (0.8217391304347826, 0.7003231017770598, 0.956953642384106, 0.808768656716418)\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate performance metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Calculate metrics for each classifier\n",
    "bernoulli_metrics = calculate_metrics(y, cross_val_predict(bernoulli_nb, X, y, cv=10))\n",
    "multinomial_metrics = calculate_metrics(y, cross_val_predict(multinomial_nb, X, y, cv=10))\n",
    "gaussian_metrics = calculate_metrics(y, cross_val_predict(gaussian_nb, X, y, cv=10))\n",
    "\n",
    "# Print the results\n",
    "print(\"Bernoulli Naive Bayes Metrics:\", bernoulli_metrics)\n",
    "print(\"Multinomial Naive Bayes Metrics:\", multinomial_metrics)\n",
    "print(\"Gaussian Naive Bayes Metrics:\", gaussian_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c272de7-5e75-4f84-8b58-aec66036e5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
